---
title: "sisbidDay2_code"
author: "Angie Boysen"
date: "July 12, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
```

Installing google sheets
```{r install}
# install.packages('googlesheets')
#library(devtools)
#install_github("jennybc/googlesheets")
library(googlesheets)
```

Reading a google sheet
```{r firstread}
sheets_url = "https://docs.google.com/spreadsheets/d/18KQQd4LY5k8Ucux1MvWCsQGQJlvd0ECTnn-3ixdOKFM/pubhtml"

gsurl1 = gs_url(sheets_url)
dat = gs_read(gsurl1)
date_accessed = date()

```

Doing the lab
```{r gslabImport}
sheets_url <- "https://docs.google.com/spreadsheets/d/103vUoftv2dLNZ_FcAz08b5ptIkWN4W2RaFn7VF39FJ8/pubhtml"
gsurl1 = gs_url(sheets_url)
dat = gs_read(gsurl1)

#read just the forst two rows
dat2cols <- gs_read(gsurl1, range = cell_cols(1:2))
```

How many students have entered data? 
``` {r gslabstudentCount}
dat2cols <- dat2cols[!is.na(dat2cols$UW),]
nStudents <- nrow(dat2cols)
nStudents
```

Do students have more than 2 years of experience with R?
```{r yearsofR}
#This way of doing it introduces NAs b/c some folks didn't enter just a number
dat$twoyears <- as.numeric(dat$`Years of R experience`)>2

```

Didn't finish the factor part
```{r factors}
# create factor "skilled" and "learning"
factorYears <- NULL
```


## doing JSON stuff now
load lib/install stuff
```{r installJSON}
# install.packages("jsonlite")
library(jsonlite)

github_url = "https://api.github.com/users/jtleek/repos"
jsonData <- fromJSON(github_url)
dim(jsonData)
names(jsonData)
```

exploring data -- 
JSON is super flexible. COlumns can have nested structure in them.
```{r exploring}
## Test whether there are nested dataframes
table(sapply(jsonData, class))
dim(jsonData$owner)
names(jsonData$owner)
```


JSON lab
```{r JSONlab}
github_url = "https://api.github.com/users/hadley/repos"
hadleyData <- fromJSON(github_url)
names(hadleyData)

##number of stars
hadleyData$stargazers_count

##number of open issues
hadleyData$open_issues
```

## Web scraping
With rvest package
Identify the URL you want to scrape
```{r webscraping1}
recount_url = "http://bowtie-bio.sourceforge.net/recount/"
# install.packages("rvest")
library(rvest)
htmlfile = read_html(recount_url)
```

We just want the table from that URL. Use the xpath for that data.
``` {r justthesubset}
nds = html_nodes(htmlfile, xpath='//*[@id="recounttab"]/table')
dat = html_table(nds)
dat = as.data.frame(dat)
head(dat)
names(dat) <- dat[1,]
dat <- dat[-1,]
head(dat)
```


## APIs - application programming interfaces
This is  way that someone exposes a large dataset to a lot of people in a way that is controled so that they get access to lots of stuff but not all the other stuff/details.
Every social media and new website usually have these.
Google "developers" and "name of website" and you'll probabl find it.
NCBI has these too! PubMed! Science stuff FTW!

Look and see if someone built a package to interface with the API. 
On rOpenSci

Example: figshare

```{r figshare}
# install.packages("rfigshare")
library(rfigshare)
```

Searching figshare for Jeff - need a figshare account for this...
``` {r chunck14}
## need a password for this part
# leeksearch = fs_search("Leek")

# length(leeksearch)
# leeksearch[[1]]
```

If you can't find a pre-built API interface
```{r githubAPI}
# install.packages("httr")
library(httr)

query_url = "https://api.github.com/search/repositories?q=created:2014-08-13+language:r+-user:cran"
req = GET(query_url)
names(content(req))
```

Example with fitbit
- go to dev.fitbit.com
- create an 'app' in r with your key and secret
- authenticate/allow accesss
- get the data you want and do what you want

## Webscraping lab
In this exercise we will be scraping all of the packages from the Bioconductor website and making a word cloud.

Go to the website: http://bioconductor.org/packages/release/BiocViews.html#___Software

Select the Software table and inspect element (on Chrome). Otherwise, go to the source and find the table by searching.
Try to use rvest as we did in class to scrape the table.
```{r webscrapLab}
bioconductor_url = "http://bioconductor.org/packages/release/BiocViews.html#___Software"
# install.packages("rvest")
library(rvest)
htmlfile = read_html(bioconductor_url)

nds = html_nodes(htmlfile, xpath='//*[@id="biocViews_package_table"]/tbody')
dat = html_table(nds)
dat = as.data.frame(dat)
head(dat)
```

This doesn't work!
Because the html code doesn't have the table in it, there is a code within the code to build that table. So we need to do lots of things that Jeff did for us...

```{r withJeffsTrick}
bioconductor_url <- "https://raw.githubusercontent.com/SISBID/Module1/gh-pages/labs/bioc-software.html"
htmlfile = read_html(bioconductor_url)

nds = html_nodes(htmlfile, xpath='//*[@id="biocViews_package_table"]')
dat = html_table(nds)
dat = as.data.frame(dat)
head(dat)
```
Now we will make the wordcloud
```{r wordcloud}
# install.packages(c("wordcloud","tm"))
library(wordcloud)
library(tm)

text <- paste(dat[,3], collapse=" ")
wordcloud(text,max.words=50)
```

